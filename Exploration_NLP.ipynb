{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchtext as tt\n",
    "import collections\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(list_sentences):\n",
    "  # create Vocab object to convert words/tokens to IDs\n",
    "  # assumes an instantiated global tokenizer exists    \n",
    "  counter_obj = collections.Counter()\n",
    "  for sentence in list_sentences:\n",
    "      sentence = sentence.strip()\n",
    "      split_and_lowered = g_toker(sentence) #global\n",
    "      counter_obj.update(split_and_lowered)\n",
    "  result = tt.vocab.vocab(counter_obj, min_freq=1, specials=('<unk>','<pad>'))\n",
    "  return result\n",
    "\n",
    "def make_data_list(DF):\n",
    "  # get all data into one big list of (label, review) tuples\n",
    "  # result will be passed to DataLoader, used by collate_fn\n",
    "  result = []\n",
    "  for x in range(0,len(DF)):\n",
    "    tpl = (DF['Sentiment'].iloc[x], DF['Phrase'].iloc[x])  # label, review\n",
    "    result.append(tpl)\n",
    "  return result\n",
    "\n",
    "def collate_data(batch):\n",
    "  # rearrange a batch and compute offsets too\n",
    "  # needs a global vocab and tokenizer\n",
    "  label_lst, review_lst, offset_lst = [], [], [0]\n",
    "  stoi = g_vocab.get_stoi()\n",
    "  for (_lbl, _rvw) in batch:\n",
    "    label_lst.append(int(_lbl))  # string to int\n",
    "  \n",
    "    rvw_idxs = [stoi[tok] for tok in g_toker(_rvw)]  # idxs\n",
    "    rvw_idxs = [g_vocab[tok] for tok in g_toker(_rvw)]  # stoi opt.\n",
    "    rvw_idxs = T.tensor(rvw_idxs, dtype=T.int64)  # to tensor\n",
    "    review_lst.append(rvw_idxs)\n",
    "    offset_lst.append(len(rvw_idxs))\n",
    "\n",
    "  label_lst = T.tensor(label_lst, dtype=T.int64).to(device) \n",
    "  offset_lst = T.tensor(offset_lst[:-1]).cumsum(dim=0).to(device) \n",
    "  review_lst = T.cat(review_lst).to(device)  # 2 tensors to 1\n",
    "\n",
    "  return (label_lst, review_lst, offset_lst)\n",
    "\n",
    "def train(net, ldr, bs, me, le, lr):\n",
    "  # network, loader, bat_size, max_epochs, log_every, lrn_rate\n",
    "  net.train()\n",
    "  opt = T.optim.SGD(net.parameters(), lr=lr)\n",
    "  loss_func = T.nn.CrossEntropyLoss()  # will apply softmax\n",
    "  print(\"\\nStarting training\")\n",
    "  for epoch in range(0, me):\n",
    "    epoch_loss = 0.0\n",
    "    for bix, (labels, reviews, offsets) in enumerate(ldr):\n",
    "      opt.zero_grad()\n",
    "      oupt = net(reviews, offsets)  # get predictions\n",
    "      loss_val = loss_func(oupt, labels)  # compute loss\n",
    "      loss_val.backward()  # compute gradients\n",
    "      epoch_loss += loss_val.item()  # accum loss for display\n",
    "      opt.step()  # update net weights\n",
    "    print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
    "  print(\"Done \")\n",
    "\n",
    "def accuracy(net, meta_lst):\n",
    "  net.eval()\n",
    "  ldr = T.utils.data.DataLoader(meta_lst, \\\n",
    "    batch_size=1, shuffle=False, collate_fn=collate_data)\n",
    "  num_correct = 0; num_wrong = 0\n",
    "  for bix, (labels, reviews, offsets) in enumerate(ldr):\n",
    "    with T.no_grad():\n",
    "      oupt = net(reviews, offsets)  # get prediction values\n",
    "    pp = T.softmax(oupt, dim=1)  # pseudo-probability\n",
    "    predicted = T.argmax(pp, dim=1)  # 0 or 1 as tensor\n",
    "    if labels.item() == predicted.item():\n",
    "      num_correct += 1\n",
    "    else:\n",
    "      num_wrong += 1\n",
    "\n",
    "  return (num_correct * 1.0) / (num_correct + num_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(T.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    self.vocab_size = len(g_vocab)\n",
    "    self.embed_dim = 50\n",
    "    self.num_class = 5\n",
    "\n",
    "    self.embed = T.nn.EmbeddingBag(self.vocab_size,\n",
    "      self.embed_dim)\n",
    "    self.fc1 = T.nn.Linear(self.embed_dim, 20)\n",
    "    self.fc2 = T.nn.Linear(20, self.num_class)\n",
    "\n",
    "    lim = 0.05\n",
    "    self.embed.weight.data.uniform_(-lim, lim)\n",
    "    self.fc1.weight.data.uniform_(-lim, lim)\n",
    "    self.fc1.bias.data.zero_()\n",
    "    self.fc2.weight.data.uniform_(-lim, lim)\n",
    "    self.fc2.bias.data.zero_()\n",
    "\n",
    "  def forward(self, reviews, offsets):\n",
    "    z = self.embed(reviews, offsets)\n",
    "    z = T.tanh(self.fc1(z))  # tanh activation\n",
    "    z = self.fc2(z)  # no activation: CrossEntropyLoss\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training\n",
      "epoch =    0   loss = 64002.6739\n",
      "epoch =    1   loss = 54891.0225\n",
      "epoch =    2   loss = 48743.5561\n",
      "epoch =    3   loss = 46076.4087\n",
      "epoch =    4   loss = 44111.1318\n",
      "Done \n"
     ]
    }
   ],
   "source": [
    "device = T.device(\"cpu\")\n",
    "\n",
    "train_set = pd.read_csv('train.tsv', sep='\\t')\n",
    "#g_ indicate global tokenizer\n",
    "g_toker = tt.data.utils.get_tokenizer(\"basic_english\")\n",
    "g_vocab = make_vocab(train_set['Phrase'])\n",
    "\n",
    "bat_size = 3\n",
    "data_lst = make_data_list(train_set)\n",
    "train_ldr = T.utils.data.DataLoader(data_lst, \\\n",
    "  batch_size=bat_size, shuffle=True, \\\n",
    "  collate_fn=collate_data)\n",
    "net = NeuralNet().to(device)\n",
    "max_epochs = 5\n",
    "log_interval = 30\n",
    "lrn_rate = 0.05\n",
    "train(net, train_ldr, bat_size, max_epochs, \\\n",
    "    log_interval, lrn_rate)\n",
    "acc_train = accuracy(net, data_lst)\n",
    "print(\"\\nAccuracy of model on training data = \\\n",
    "  %0.4f \" % acc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New movie review: Overall, I liked the film.\n",
      "Sentiment prediction probabilities [neg, pos]: \n",
      "tensor([[0.0498, 0.2193, 0.4741, 0.2048, 0.0521]])\n"
     ]
    }
   ],
   "source": [
    "print(\"New movie review: Overall, I liked the film.\")\n",
    "review_lst = [(\"-1\", \"Overall, I liked the film.\")] \n",
    "ldr = T.utils.data.DataLoader(review_lst, \\\n",
    "  batch_size=1, shuffle=True, collate_fn=collate_data)\n",
    "net.eval()\n",
    "(_, review, offset) = iter(ldr).next()\n",
    "with T.no_grad():\n",
    "  oupt = net(review, offset)  # get raw prediction values\n",
    "pp = T.softmax(oupt, dim=1)   # as pseudo-probabilities\n",
    "print(\"Sentiment prediction probabilities [neg, pos]: \")\n",
    "print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e34fa988cd04c5961e9adb7490cb3dda91dc4d4e367140585033e616a1e5d8f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
